{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet_regressor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDnJaIvvwpgt3QHadftXgf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pufeiyang/Deep_Learning_Regression/blob/main/DenseNet_regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DenseNet\n"
      ],
      "metadata": {
        "id": "1hCB1m1AQw-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"DenseNet 1DCNN in Tensorflow-Keras\n",
        "Reference: Densely Connected Convolutional Networks [https://arxiv.org/abs/1608.06993]\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def Conv_1D_Block(x, model_width, kernel, strides):\n",
        "    # 1D Convolutional Block with BatchNormalization\n",
        "    x = tf.keras.layers.Conv1D(model_width, kernel, strides=strides, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def stem(inputs, num_filters):\n",
        "    # Construct the Stem Convolution Group\n",
        "    # inputs : input vector\n",
        "    conv = Conv_1D_Block(inputs, num_filters, 7, 2)\n",
        "    if conv.shape[1] <= 2:\n",
        "        pool = tf.keras.layers.MaxPooling1D(pool_size=1, strides=2, padding=\"same\")(conv)\n",
        "    else:\n",
        "        pool = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(conv)\n",
        "\n",
        "    return pool\n",
        "\n",
        "\n",
        "def conv_block(x, num_filters, bottleneck=True):\n",
        "    # Construct Block of Convolutions without Pooling\n",
        "    # x        : input into the block\n",
        "    # n_filters: number of filters\n",
        "    if bottleneck:\n",
        "        num_filters_bottleneck = num_filters * 4\n",
        "        x = Conv_1D_Block(x, num_filters_bottleneck, 1, 1)\n",
        "\n",
        "    out = Conv_1D_Block(x, num_filters, 3, 1)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def dense_block(x, num_filters, num_layers, bottleneck=True):\n",
        "    for i in range(num_layers):\n",
        "        cb = conv_block(x, num_filters, bottleneck=bottleneck)\n",
        "        x = tf.keras.layers.concatenate([x, cb], axis=-1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transition_block(inputs, num_filters):\n",
        "    x = Conv_1D_Block(inputs, num_filters, 1, 2)\n",
        "    if x.shape[1] <= 2:\n",
        "        x = tf.keras.layers.AveragePooling1D(pool_size=1, strides=2, padding=\"same\")(x)\n",
        "    else:\n",
        "        x = tf.keras.layers.AveragePooling1D(pool_size=2, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def classifier(inputs, class_number):\n",
        "    # Construct the Classifier Group\n",
        "    # inputs       : input vector\n",
        "    # class_number : number of output classes\n",
        "    out = tf.keras.layers.Dense(class_number, activation='softmax')(inputs)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def regressor(inputs, feature_number):\n",
        "    # Construct the Regressor Group\n",
        "    # inputs         : input vector\n",
        "    # feature_number : number of output features\n",
        "    out = tf.keras.layers.Dense(feature_number, activation='linear')(inputs)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class DenseNet:\n",
        "    def __init__(self, length, num_channel, num_filters, problem_type='Regression',\n",
        "                 output_nums=1, pooling='avg', dropout_rate=False, bottleneck=True):\n",
        "        self.length = length\n",
        "        self.num_channel = num_channel\n",
        "        self.num_filters = num_filters\n",
        "        self.problem_type = problem_type\n",
        "        self.output_nums = output_nums\n",
        "        self.pooling = pooling\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.bottleneck = bottleneck\n",
        "\n",
        "    def MLP(self, x):\n",
        "        if self.pooling == 'avg':\n",
        "            x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "        elif self.pooling == 'max':\n",
        "            x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "        # Final Dense Outputting Layer for the outputs\n",
        "        x = tf.keras.layers.Flatten(name='flatten')(x)\n",
        "        if self.dropout_rate:\n",
        "            x = tf.keras.layers.Dropout(self.dropout_rate, name='Dropout')(x)\n",
        "        outputs = tf.keras.layers.Dense(self.output_nums, activation='linear')(x)\n",
        "        if self.problem_type == 'Classification':\n",
        "            outputs = tf.keras.layers.Dense(self.output_nums, activation='softmax')(x)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def DenseNet121(self):\n",
        "        inputs = tf.keras.Input((self.length, self.num_channel))  # The input tensor\n",
        "        stem_block = stem(inputs, self.num_filters)  # The Stem Convolution Group\n",
        "        Dense_Block_1 = dense_block(stem_block, self.num_filters * 2, 6, bottleneck=self.bottleneck)\n",
        "        Transition_Block_1 = transition_block(Dense_Block_1, self.num_filters)\n",
        "        Dense_Block_2 = dense_block(Transition_Block_1, self.num_filters * 4, 12, bottleneck=self.bottleneck)\n",
        "        Transition_Block_2 = transition_block(Dense_Block_2, self.num_filters)\n",
        "        Dense_Block_3 = dense_block(Transition_Block_2, self.num_filters * 8, 24, bottleneck=self.bottleneck)\n",
        "        Transition_Block_3 = transition_block(Dense_Block_3, self.num_filters)\n",
        "        Dense_Block_4 = dense_block(Transition_Block_3, self.num_filters * 16, 16, bottleneck=self.bottleneck)\n",
        "        outputs = self.MLP(Dense_Block_4)\n",
        "        # Instantiate the Model\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def DenseNet161(self):\n",
        "        inputs = tf.keras.Input((self.length, self.num_channel))  # The input tensor\n",
        "        stem_block = stem(inputs, self.num_filters)  # The Stem Convolution Group\n",
        "        Dense_Block_1 = dense_block(stem_block, self.num_filters * 2, 6, bottleneck=self.bottleneck)\n",
        "        Transition_Block_1 = transition_block(Dense_Block_1, self.num_filters * 2)\n",
        "        Dense_Block_2 = dense_block(Transition_Block_1, self.num_filters * 4, 12, bottleneck=self.bottleneck)\n",
        "        Transition_Block_2 = transition_block(Dense_Block_2, self.num_filters * 4)\n",
        "        Dense_Block_3 = dense_block(Transition_Block_2, self.num_filters * 8, 36, bottleneck=self.bottleneck)\n",
        "        Transition_Block_3 = transition_block(Dense_Block_3, self.num_filters * 8)\n",
        "        Dense_Block_4 = dense_block(Transition_Block_3, self.num_filters * 16, 24, bottleneck=self.bottleneck)\n",
        "        outputs = self.MLP(Dense_Block_4)\n",
        "        # Instantiate the Model\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def DenseNet169(self):\n",
        "        inputs = tf.keras.Input((self.length, self.num_channel))  # The input tensor\n",
        "        stem_block = stem(inputs, self.num_filters)  # The Stem Convolution Group\n",
        "        Dense_Block_1 = dense_block(stem_block, self.num_filters * 2, 6, bottleneck=self.bottleneck)\n",
        "        Transition_Block_1 = transition_block(Dense_Block_1, self.num_filters * 2)\n",
        "        Dense_Block_2 = dense_block(Transition_Block_1, self.num_filters * 4, 12, bottleneck=self.bottleneck)\n",
        "        Transition_Block_2 = transition_block(Dense_Block_2, self.num_filters * 4)\n",
        "        Dense_Block_3 = dense_block(Transition_Block_2, self.num_filters * 8, 32, bottleneck=self.bottleneck)\n",
        "        Transition_Block_3 = transition_block(Dense_Block_3, self.num_filters * 8)\n",
        "        Dense_Block_4 = dense_block(Transition_Block_3, self.num_filters * 16, 32, bottleneck=self.bottleneck)\n",
        "        outputs = self.MLP(Dense_Block_4)\n",
        "        # Instantiate the Model\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def DenseNet201(self):\n",
        "        inputs = tf.keras.Input((self.length, self.num_channel))  # The input tensor\n",
        "        stem_block = stem(inputs, self.num_filters)  # The Stem Convolution Group\n",
        "        Dense_Block_1 = dense_block(stem_block, self.num_filters * 2, 6, bottleneck=self.bottleneck)\n",
        "        Transition_Block_1 = transition_block(Dense_Block_1, self.num_filters)\n",
        "        Dense_Block_2 = dense_block(Transition_Block_1, self.num_filters * 4, 12, bottleneck=self.bottleneck)\n",
        "        Transition_Block_2 = transition_block(Dense_Block_2, self.num_filters)\n",
        "        Dense_Block_3 = dense_block(Transition_Block_2, self.num_filters * 8, 48, bottleneck=self.bottleneck)\n",
        "        Transition_Block_3 = transition_block(Dense_Block_3, self.num_filters)\n",
        "        Dense_Block_4 = dense_block(Transition_Block_3, self.num_filters * 16, 32, bottleneck=self.bottleneck)\n",
        "        outputs = self.MLP(Dense_Block_4)\n",
        "        # Instantiate the Model\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def DenseNet264(self):\n",
        "        inputs = tf.keras.Input((self.length, self.num_channel))  # The input tensor\n",
        "        stem_block = stem(inputs, self.num_filters)  # The Stem Convolution Group\n",
        "        Dense_Block_1 = dense_block(stem_block, self.num_filters * 2, 6, bottleneck=self.bottleneck)\n",
        "        Transition_Block_1 = transition_block(Dense_Block_1, self.num_filters * 2)\n",
        "        Dense_Block_2 = dense_block(Transition_Block_1, self.num_filters * 4, 12, bottleneck=self.bottleneck)\n",
        "        Transition_Block_2 = transition_block(Dense_Block_2, self.num_filters * 4)\n",
        "        Dense_Block_3 = dense_block(Transition_Block_2, self.num_filters * 8, 64, bottleneck=self.bottleneck)\n",
        "        Transition_Block_3 = transition_block(Dense_Block_3, self.num_filters * 8)\n",
        "        Dense_Block_4 = dense_block(Transition_Block_3, self.num_filters * 16, 48, bottleneck=self.bottleneck)\n",
        "        outputs = self.MLP(Dense_Block_4)\n",
        "        # Instantiate the Model\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     # Configurations\n",
        "#     length = 1024  # Length of each Segment\n",
        "#     model_name = 'DenseNet201'  # DenseNet Models\n",
        "#     model_width = 16 # Width of the Initial Layer, subsequent layers start from here\n",
        "#     num_channel = 1  # Number of Input Channels in the Model\n",
        "#     problem_type = 'Regression' # Classification or Regression\n",
        "#     output_nums = 1  # Number of Class for Classification Problems, always '1' for Regression Problems\n",
        "#     #\n",
        "#     Model = DenseNet(length, num_channel, model_width, problem_type=problem_type, output_nums=output_nums, pooling='avg', dropout_rate=False, bottleneck=True).DenseNet201()\n",
        "#     Model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss=tf.keras.losses.MeanAbsoluteError(), metrics=tf.keras.metrics.MeanSquaredError())\n",
        "#     Model.summary()"
      ],
      "metadata": {
        "id": "ZYi2T1GCNtp2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test GPU"
      ],
      "metadata": {
        "id": "FTpx6sZQJR1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxXd9Se2JQ1M",
        "outputId": "330769e6-521d-4b44-bc52-edf0c68fb3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA enabled GPU Available? False\n",
            "GPU Number: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cc1dfb221668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Is CUDA enabled GPU Available?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU Number:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current GPU Index:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU Type:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU Capability:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# print(\"Is CUDA enabled GPU Available?\", torch.cuda.is_available())\n",
        "# print(\"GPU Number:\", torch.cuda.device_count())\n",
        "# print(\"Current GPU Index:\", torch.cuda.current_device())\n",
        "# print(\"GPU Type:\", torch.cuda.get_device_name(device=None))\n",
        "# print(\"GPU Capability:\", torch.cuda.get_device_capability(device=None))\n",
        "# print(\"Is GPU Initialized yet?\", torch.cuda.is_initialized())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Libraries"
      ],
      "metadata": {
        "id": "leZDrT_vMV-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score, roc_curve, plot_roc_curve\n",
        "from sklearn.metrics import confusion_matrix, classification_report, auc, precision_recall_curve, average_precision_score\n",
        "sns.set_theme(style=\"whitegrid\")"
      ],
      "metadata": {
        "id": "jrVsvb1-LOBz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Dataset "
      ],
      "metadata": {
        "id": "QVWuaQxitL8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/dataset.xlsx')\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgYSCgc5tUXa",
        "outputId": "186b79f4-5e4f-4641-ab23-358f76dc1e1a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-61937838d463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dataset.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[0;32m-> 1192\u001b[0;31m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m                 )\n\u001b[1;32m   1194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     with get_handle(\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     ) as handle:\n\u001b[1;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Test Split"
      ],
      "metadata": {
        "id": "bhXVjGs5uDhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = df.iloc[:,3:].values\n",
        "y_data = df.iloc[:,2].values\n",
        "\n",
        "print(x_data.shape)\n",
        "print(y_data.shape)"
      ],
      "metadata": {
        "id": "7-8dcZTJyAOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.20, random_state=42)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "l5xGklQKyZwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.expand_dims(x_train, axis=2)\n",
        "x_test = np.expand_dims(x_test, axis=2)\n",
        "\n",
        "print(x_train.shape, x_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "4J8CD0VP45ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and Train Imported Data using the Regression Model"
      ],
      "metadata": {
        "id": "yaeEbL0oBSGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Configurations for DenseNet in Regression Mode\"\n",
        "length = x_train.shape[1]   # Number of Features (or length of the signal)\n",
        "model_width = 32            # Number of Filter or Kernel in the Input Layer (Power of 2 to avoid error)\n",
        "num_channel = 1             # Number of Input Channels\n",
        "problem_type = 'Regression' # Regression or Classification\n",
        "output_number = 1           # Number of Outputs in the Regression Mode - 1 input is mapped to a single output"
      ],
      "metadata": {
        "id": "VkrLo99IBha-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Regression_Model = DenseNet(length, num_channel, model_width, problem_type=problem_type, output_nums=output_number).DenseNet169() # Build Model\n",
        "Regression_Model.compile(loss='mae', optimizer='adam', metrics= ['mse']) # Compile Model"
      ],
      "metadata": {
        "id": "Qg7OtUUmB2ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Regression_Model.summary() # Summary of the Model"
      ],
      "metadata": {
        "id": "tRGx58f8CHL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping and Model_Checkpoints are optional parameters\n",
        "# Early Stopping is to stop the training based on certain condition set by the user\n",
        "# Model Checkpoint is to save a model in a directory based on certain conditions so that it can be used later for Transfer Learning or avoiding retraining\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min'), \n",
        "             tf.keras.callbacks.ModelCheckpoint('trained_models/DenseNet169_'+str(model_width)+'_'+str(num_channel)+'_'+str(output_number)+'_'+str(problem_type)+'.h5', \n",
        "                                                verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n",
        "history = Regression_Model.fit(x_train, y_train, epochs=500, batch_size=128, verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks)\n",
        "# Save 'History' of the model for model performance analysis performed later"
      ],
      "metadata": {
        "id": "bFsWO6EdC9yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Predictions = Regression_Model.predict(x_test, verbose=1)\n",
        "print(Predictions.shape)"
      ],
      "metadata": {
        "id": "608So8bxHPsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error of the prediction, one of many evaluation metrics\n",
        "# Using Mean Absolute Error (MAE) in this case as a sample\n",
        "Error = mean_absolute_error(y_test, Predictions)\n",
        "print(f\"MAE: {Error}\")"
      ],
      "metadata": {
        "id": "HXXKPgk4Hc8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def history_plot(history):\n",
        "  # list all dictionaries in history\n",
        "  print(history.history.keys())\n",
        "  # summarize history for error\n",
        "  plt.figure(figsize=(12,10))\n",
        "  plt.subplot(2,1,1)\n",
        "  plt.plot(history.history['mse'])\n",
        "  plt.plot(history.history['val_mse'])\n",
        "  plt.title('Model Error Performance')\n",
        "  plt.ylabel('Error')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='upper right')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.figure(figsize=(12,10))\n",
        "  plt.subplot(2,1,2)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='upper right')\n",
        "  plt.show()\n",
        "#\n",
        "history_plot(history)"
      ],
      "metadata": {
        "id": "j5M8qt2KHjzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_prediction(test_labels, test_predictions):\n",
        "  plt.figure(figsize=(15,10))\n",
        "  plt.scatter(test_labels, test_predictions)\n",
        "  '''Add Trendline'''\n",
        "  z = np.polyfit(test_labels.ravel(), test_predictions.ravel(), 1)\n",
        "  p = np.poly1d(z)\n",
        "  plt.plot(test_labels, p(test_labels))\n",
        "  plt.text(np.max(test_labels)/3,np.max(test_predictions),f'y = {p[1]:.2f}x+{p[0]:.2f}', fontsize=15)\n",
        "  plt.title('Ground Truth vs. Prediction Scatter Plot', fontsize=20)\n",
        "  plt.xlabel('Ground Truth', fontsize=15)\n",
        "  plt.ylabel('Predictions', fontsize=15)\n",
        "#\n",
        "plot_prediction(y_test, Predictions)"
      ],
      "metadata": {
        "id": "AZzqb4mpJXyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data = {'Ground Truth': y_test.ravel(), 'Predictions': Predictions.ravel()})\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.set_style('whitegrid')\n",
        "sns.kdeplot(data=df)\n",
        "plt.title('Kernel Density Estimation (KDE) Plot for Ground Truth and Predictions', fontsize=20)\n",
        "plt.xlabel('Magnitude', fontsize=15)\n",
        "plt.ylabel('Density', fontsize=15)"
      ],
      "metadata": {
        "id": "QbJBvL0yJbAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,10))\n",
        "df = pd.DataFrame(data = {'Ground Truth': y_test.ravel(), 'Predictions': Predictions.ravel()})\n",
        "ax = sns.violinplot(data=df)\n",
        "plt.title('Violin Plot for Ground Truth and Predictions', fontsize=20)\n",
        "plt.ylabel('Magnitude', fontsize=15)"
      ],
      "metadata": {
        "id": "osaulrjjKM6C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}